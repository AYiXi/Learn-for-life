### *2020-11-02*
- [REALI]
  - 解决 redis 内存达到最大值的问题
    - `OOM command not allowed when used memory > 'maxmemory'`
      - redis 用的是集群, 总共2G, 单个节点 2G / 8 容量满了, 举个例子, redis 容量是八个桶, 其中一个桶容量爆了, 而新加入的key如果恰好进入这个桶, 这出现 OOM(out of memory) 错误, 如果key恰好进入其他的桶, 则无异常. 表面看上去容量还没满, 但有时候key报错
  - Vmware Fusuin 激活码 `XKZYV-PK9CC-A1Y0X-K5HZL-Y65ZV`
  - 更新微信公众号的历史文章(大概7个用户)
  - 完成 `qq_41385102` 用户的个人博客搬家(80)
  - 完成 `u012168125` 用户的个人博客搬家(24)
  - 完成 `weixin_47166516` 用户的个人博客搬家(22)
  - 完成 `u011700186` 用户的个人博客搬家(114)
  - 完成 `ptwenzi` 用户的个人博客搬家(145)
  - 与靳俊昭商议接口事宜
  - 修改 `move_blink_spider` 一些不对的地方
    - 为了保证正确, 应该在最后处判断 `type`
    ```py
    # 确保 type 正确
    if not ((type_ == 1 and 'text' in ct) or
            (type_ == 2 and 'picList' in ct) or
            (type_ == 4 and 'video_id' in ct)):
        raise TypeError('type 不在规定范围')
    ```
- [LEARN]
  - Mac 虚拟机安装 Win10
  - [npm 更换淘宝镜像源](https://blog.csdn.net/u012780176/article/details/82586358)
    - npm config set registry https://registry.npm.taobao.org
    - npm config get registry

### *2020-11-03*
- [REALI]
  - 更新微信公众号的文章
  - 更新 blink 项目的一些入库的判断逻辑
    - 对知乎公式代码图片做过滤处理
  - 给张欣羽讲解一些知识
  - 配合杨赛做博文插入的调试
  - 配合靳俊昭做 blink 数据插入的调试
- [LEARN]
  - 了解学习 optparse
  ```py
  from optparse import OptionParser

  parser = OptionParser()
  parser.add_option("-f", "--file", dest="filename",
                    help="write report to FILE", metavar="FILE")
  parser.add_option("-q", "--quiet",
                    action="store_false", dest="verbose", default=True,
                    help="don't print status messages to stdout")

  print(parser.parse_args()) 
  ```

### *2020-11-04*
- [REALI]
  - 更新微信公众号数据
  - [BloomFilter](https://www.jianshu.com/p/e4773b69319d)
  - 更改 `scrapy.cfg` 位置, 需要更新的地方如下:
  ```py
  # scrapy_settings.py
  SPIDER_MODULES = ['move_blink_spider.scrapy_projects.spiders']

  NEWSPIDER_MODULE = 'move_blink_spider.scrapy_projects.spiders'

  # scrapy.cfg
  [settings]
  default = move_blink_spider.scrapy_projects.scrapy_settings

  [deploy]
  #url = http://localhost:6800/
  project = move_blink_spider.scrapy_projects
  ```
  - scrapy 在找不到 `scrapy.cfg` 的情况下会向父级去寻找
  - `make_request_from_data` 的重写, 从 redis 读取 key
  ```py
  def make_request_from_data(self, data):
      key = data.decode()
      self.logger_loguru.info(f'{key} | 开始抓取')
      return Request(
          self.query_url.format(1, key),
          callback=self.parse_json_page,
          meta={'page': 1, 'key': key}
      )
  ```
  - 完成 B站 通过用户主页查询视频信息的爬虫
- [LEARN]
  - `scrapy.cfg` 在不同目录下面的配置

### *2020-11-05*
- [REALI]
  - 完成微信公众号用户更新
  - 开发 blink
    - B站 视频分布式下载上传
    - 微博视频爬虫调研
    - 其他结构性调整
- [LEARN]
  - [Scrapyd使用详解](https://juejin.im/post/6844903939041689613)
  - adb -s [xxx] shell (-s 一定在 shell 之前)
  - redis pipeline
  ```py
  def get_rows_from_redis(self, fetch=None):
      rows = self.redis.spop(VIDEO_DISTRIBUTE_KEY.format(self.site), fetch or self.fetch)
      return [json.loads(row) for row in rows]

  def add_rows_to_redis(self, fetch=None):
      pipeline = self.redis.pipeline()
      rows = self.get_rows_from_mysql(TYPE_CODE_VIDEO_OFFLINE, fetch or -1)
      for row in  rows:
          pipeline.sadd(VIDEO_DISTRIBUTE_KEY.format(self.site), json.dumps(row))
      pipeline.execute()
  ```
  - 安卓逆向