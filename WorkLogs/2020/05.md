### *2020-05-06*
[LEARN]
- 学习 scrapy 下载中间件, 重试次数, 自定义插件, 之间的关系以及错误处理
- 公司培训, 学习了解公司的业务逻辑

### *2020-05-07*
[REALI]
- 解析粉笔课堂的接口
- 完成通过 ip 查询地址的爬虫, 从 hive 查询, 入库到 mongodb, 大概 40k 数据量
- 开公司介绍会, 更加了解公司的业务逻辑和发展方向
  
[LEARN]
- 逐渐了解 scrapy 的日志信息
- 知道 scrapy 的 meta 里面带有包含 代理, 下载时间等参数
- 在使用列表一查询某个字段在列表二中是否存在时候, 如果先把列表二转换为字典, 耗时几乎缩短 99.9%
```py
@time_use
def get_list_item(items, mongo_ips):
    d = [item for item in items if item['_ip'] not in mongo_ips]
    return d

@time_use
def get_dict_item(items, mongo_ips):
    mongo_ips = {mongo_ip: True for mongo_ip in mongo_ips}
    d = [item for item in items if not mongo_ips.get(item['_ip'], None)]
    return d

# len(items) = 44100, len(mongo_ips) = 40016

# [FUNCTION] get_list_item USE TIME: 29.350s
# len(d) = 94

# [FUNCTION] get_dict_item USE TIME: 0.000s
# len(d) = 94
```
- 对 pipelines 里面判断对优化化简, 并且再同时执行百万次对情况下节约了 30-50% 的时间
```python
# 以下两个函数等价
def item_name(item):
    name = {
        ElePoiidItem        : 'all_poiid',
        EleRestaurantItem   : 'restaurant_new',
        ElePoiLatLogItem    : 'restaurant_lat_log',
        ElePoiLatLogItemTest: 'restaurant_test',
        EleCityItem         : 'city',
        EleCityTestItem     : 'city_test',
        EleStoredMenuItem   : 'menu'
    }

    collection_name = name[item.__class__]

def item_name(item):
    if isinstance(item, ElePoiidItem):
        collection_name = 'all_poiid'
    elif isinstance(item, EleRestaurantItem):
        collection_name = 'restaurant_new'
    elif isinstance(item, ElePoiLatLogItem):
        collection_name = 'restaurant_lat_log'
    elif isinstance(item, ElePoiLatLogItemTest):
        collection_name = 'restaurant_test'
    elif isinstance(item, EleCityItem):
        collection_name = 'city'
    elif isinstance(item, EleCityTestItem):
        collection_name = 'city_test'
    else:
        collection_name = 'menu'
```

### *2020-05-08*
[REALI]
- 参加公司开会, 了解公司业务流程和业务逻辑
- 查看 ele_new 的项目代码

[LEARN]
- 学会建立, 查询 mongodb 的索引
- 学会 mongodb 的 CRUD 操作

### *2020-05-09*
[REALI]
- 参加公司开会, 了解公司业务流程和业务逻辑
- 查看调试 redis 数据重复的问题
- 熟悉 mongodb 对查询方式

### *2020-05-11*
- [REALI]
  - 阅读 scrapy [源码](http://kaito-kidd.com/2016/11/01/scrapy-code-analyze-architecture/), 详细了解了各个部件的相互调用逻辑, 但是源码很多地方还看不懂
  - 阅读小红书项目源码
    - 安装 Crypto 模块
      - pip install pycrypto
      - crypto --> Crypto
  - 从百观科技离职

- [LEARN]
  - 在 for 循环中有函数时不会重复调用
  - scrapy 获取 settings.py 的方法
  ```py
  from scrapy.utils.project import get_project_settings
  settings = get_project_settings()

  or 

  settings = spider.settings
  ```

- [FIXED]
  - 修复 redis 入库重复的 bug
